\documentclass[10pt]{article}
\usepackage{url}
\usepackage{sober}
\usepackage{color}
\usepackage{times}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{fullpage}
\newcommand{\qq}[1]{\color{blue} #1 \color{black}}
\title{Multivariate linear (and affine) discrete-time deterministic models}
\author{\copyright\ Steve Walker \today}
\date{}
\begin{document}
\maketitle

\enlargethispage{20pt}
\thispagestyle{empty}
\begin{multicols}{2}


<<include=FALSE>>=
opts_chunk$set(height=2.7,fig.width=3)
@

 \setkeys{Gin}{width=3in} 
  
 Basic model: $\bm x(t+1) = \bm A \bm x(t)$. For example,
 juvenile/adult model: fractions $\{s_J,s_A\}$ of juveniles and adults
 survive; adults have $f$ offspring each (on average); surviving
 juveniles become adults.  So $A(t+1)=s_AA(t)+s_JJ(t)$, $J(t+1)=fA(t)$
 \emph{or} $A(t+1)=s_A A(t) + s_JfA(t-1)$. This can be written as a
 matrix equation, 
 \begin{equation*}
   \begin{bmatrix}
     J(t+1) \\ A(t+1)
   \end{bmatrix} = 
   \begin{bmatrix}
     0 & f \\ s_J & s_A
   \end{bmatrix} 
   \begin{bmatrix}
     J(t) \\ A(t)
   \end{bmatrix}
 \end{equation*}
    
\section*{Fixed points}

$\bm x{\star}$ is a fixed point if $\bm x{\star} = \bm A \bm
x{\star}$, $\bm 0 = (\bm A - \bm I) \bm x{\star}$ where $\bm I$ is the
identity matrix. The null space of $\bm A - \bm I$ has all the fixed
points. If $\bm A - \bm I$ is invertible, we find $\bm 0 = (\bm A -
\bm I )^{-1}(\bm A - \bm I) \bm x{\star}$, which implies $\bm 0 = \bm
I \bm x{\star}$, or $\bm x{\star} = \bm 0$. However, if $\bm A - \bm
I$ is not invertible, there is an $n-r$ dimensional space of
fixed-points, where $n$ is the number of rows/columns in $\bm A - \bm
I$ and $r$ is the rank of that matrix. A helpful trick is that a
matrix is invertible if its determinant is non-zero. For example, the
determinant of the juvenile-adult model is $-fs_J$, which is not zero
and so the only fixed point is at the origin.

In \texttt{R}, the rank, inverse, and determinant of a matrix
\texttt{B} is given by \texttt{qr(B)\$rank}, \texttt{solve(B)}, and
\texttt{det(B)}

\section*{Time-dependent solution}

%% A(t+2) = s_A A(t+1) + s_J fA(t)
%%        = s_A(s_A A(t)+ s_J f A(t-1)) +s_J fA(t) 
%%        = (s^2_A +s_J f)A(t) + s_A s_J f A(t-1)
%% A(t+3) = s_A A(t+2) + s_J fA(t+1)
%%        = (s^3_A + s_A s_J f) A(t) + s_A^2 s_J f A(t-1) + s_J f (s_A A(t) + s_J f A(t-1))
%%        = (s^3_A + 2 s_A s_J f) A(t) + (s_A^2 s_J f + s_J^2 f) A(t-1)
%%  ... ugh ...
Four approaches:
\begin{description}
\item[recurrsion] $\bm x(1) = \bm A \bm x(0)$, then $\bm x(2) =
  \bm{AA} \bm x(0)$, and in general $\bm x(t) = \underbrace{\bm A
    ... \bm A}_{t-\text{times}} \bm x(0)$.
  \item[matrix powers] Let's define matrix powers, so that $\bm x(t) =
    \bm A^t \bm x(0)$. However, this method doesn't provide much
    insight. 
  \item[diagonalization] Can get insight by diagonalizing $\bm A =
    \bm{SDS}^{-1}$, where $\bm S$ is a matrix whose columns are the
    eigenvectors of $\bm A$ and $\bm D$ is a matrix with eigenvalues
    on the diagonal and zeros everywhere else. Subbing this into the
    matrix power equation, $\bm x(t) = \left(\bm S \bm D \bm
      S^{-1}\right)^t \bm x(0) = \underbrace{\bm S \bm D \bm S^{-1}
      \bm S \bm D \bm S^{-1} ...  \bm S \bm D \bm
      S^{-1}}_{t-\text{times}} \bm x(0) = \underbrace{\bm S \bm D^{t}
      \bm S^{-1}}_{t-\text{times}} \bm x(0)$, because the $\bm S^{-1}
    \bm S$ terms cancel. 
   \item[series] Let $\bm c = \bm S^{-1} \bm x(0)$. This
    allows us to write $\bm x(t) = \sum_i c_i d_i^t \bm v_i$, where
    $c_i$, $d_i$, and $\bm v_i$ is the $i$th element of $\bm c$,
    eigenvalue, and eigenvector respectively. This form also lets us
    see the importance of the dominant eigenvalue (i.e. eigenvalue
    with largest absolute value) -- because all the eigenvalues get
    raised to the power of time, as time increases all other terms
    except for the dominant become neglible. Therefore, for $t$
    sufficiently large, $\bm x(t) \approx c_1 d_1^t v_1$, where $d_1$
    is the dominant eigenvalue. \qq{What happens when $d_1 = 1$?}
    \qq{What happens when $d_1 = d_2$? Try it out in \texttt{R}}.
   \item[change of variables] Let $\bm y(t) = \bm S^{-1} \bm
     x(t)$. Then the model becomes $\bm y(t+1) = \bm D \bm y(t)$. But
     since $\bm D$ is diagonal, this model is exceptionally
     simple. It is actually just a bunch of decoupled univariate
     models \qq{(Why?)}and you know how to handle those.
\end{description}

\subsection*{Eigen-tips (mostly for the 2 by 2 case)}

If $\bm A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} &
  a_{22} \end{bmatrix}$, determinant is $\Delta = a_{11}a_{22} -
  a_{12}a_{21}$, trace is $T = a_{11} + a_{22}$, and eigen values obey
  $d_1 + d_2 = T$ and $d_1 d_2 = \Delta$. This leads to the
  characteristic polynomial $d_i^2 - Td_i + \Delta$. And so the
  eigenvalues obey $d_i = \frac{T \pm \sqrt{T^2 -
      4\Delta}}{2}$. Finally, if $\bm v_i$ and $d_i$ are an
  eigenvector/eigenvalue pair for $\bm A$, then $\bm A \bm v_i = d_i
  \bm v_i$ (i.e. a matrix and a single scalar value to the same thing
  to an eigenvector!).
  
  Example: for the juvenile-adult model, we have $d_i = \frac{s_A
    \pm \sqrt{s_A^2 + 4s_Jf}}{2}$. For each eigenvalue, solve $\begin{bmatrix}
     0 & f \\ s_J & s_A \end{bmatrix} \begin{bmatrix} v_1 \\
     v_2 \end{bmatrix} = d \begin{bmatrix} v_1 \\
     v_2 \end{bmatrix}$ to find the eigen vectors. For the dominant
   eigenvalue, this is,
   \begin{equation*}
     \begin{split}
       fv_2 = & \frac{s_A + \sqrt{s_A^2 + 4s_Jf}}{2} v_1 \\
       s_Jv_1 + s_Av_2 = & \frac{s_A + \sqrt{s_A^2 + 4s_Jf}}{2} v_2
     \end{split}
   \end{equation*}
   Could keep going but you get the idea. \qq{Simplify this system.}
   \qq{Do the same for the other eigenvalue.} \qq{Write down a
     time-dependent solution for this model with your computations.}
   \qq{What are the conditions for stability of the fixed point at the
     origin?}
   
\section*{Affine model}

Multivariate bucket/line-up: $\bm x(t+1) = \bm b + \bm A \bm
x(t)$. For fixed points solve $\bm x{\star} = \bm b + \bm A \bm
x{\star}$. If $\bm A - \bm I$ is invertible, then the solution is $\bm
x{\star} = (\bm A - \bm I)^{-1} \bm b$. Same stability conditions as
in the linear case. \qq{Can you reparameterize this model such that
  the fixed point is a parameter? It would be nice to just read off
  the fixed point woudn't it?}
%$\bm x(t+1) = \bm A (\bm x(t) - \hat{\bm x})$

\end{multicols}
\end{document}
